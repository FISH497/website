---
title: "Getting data in R"
author: "Mark Scheuerell"
date: "25 January 2021"
output:
  html_document:
    theme: readable
    highlight: textmate
    toc: false
    toc_float: true
    toc_depth: 3
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.align = "center", out.width = '90%')
```

Before we begin, you will need to install
these packages

```{r,eval=FALSE}
install.packages("jsonlite")
install.packages("rvest")
```

Now we load a few R packages

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(jsonlite)
library(rvest)
```

# Motivation

Today we are going to talk about getting data, examples of common data formats, and useful tools to access data. 

# Working directories

One of the keys to creating a reproducible workflow is maintaining the proper folder/directory structures across computers, platforms, etc. 

## Absolute paths

In the early days of **R** programming, many people worked with *absolute* paths. When you open up someone's **R** code or analysis, you would often see the `setwd()` function being used. This explicitly tells **R** to change the absolute path to the folder/directory where work is being done. Although this may work fine for an individual who works by themselves on a single computer, it will quickly lead to headaches and long trails of work when projects are shared with others.

For example, if I had a series of scripts scattered throughout a repo, and each of them began with something like 

```{r setwd_ex, eval=FALSE}
setwd("~/Users/Mark/Dcouments/folder/that/only/Mark/has")
```

you would need to go into each script and edit every single one of those *absolute* paths, such that they worked on your own computer.

## Relative paths

An alternative to using absolute paths is to use relative paths. For example, the following command would set the working directory to the `/data/` folder in someone's root directory.

```{r setwd_relative, eval=FALSE}
setwd("~/data/")
```

### RStudio `.Rproj` files

We have already seen how to set up projects in **RStudio** and make use of the relative file paths associated with a project's `.Rproj` file. When you start a project by opening a `.Rproj` file, **RStudio** automatically changes the path to the folder/directory where the `.Rproj` file is located. After opening up a `.Rproj` file, you can test this with

```{r chk_wd, eval=FALSE}
getwd()
```

### The `here` package

Within **R**, the [here](https://github.com/r-lib/here) package will automatically identify the top-level directory of a **Git** repo, and then direct all other paths relative to that. For more on project-oriented workflows, check out [this blog post](https://www.tidyverse.org/articles/2017/12/workflow-vs-script/) by Jenny Bryan.

Instead of using `setwd()` at the top your `.R` or `.Rmd` file, Jenny suggests: 

* Organize each logical project into a folder on your computer (i.e., use a research compendium).

* Make sure the top-level folder advertises itself as such. This can be as simple as having an empty file named `.here`. If you use **RStudio** and/or **Git**, those both leave characteristic files behind that will get the job done.

* Use the `here()` function from the `here` package to build the path when you read or write a file. Create paths relative to the top-level directory.

* Whenever you work on this project, launch the **R** process from the project's top-level directory. If you launch **R** from the shell, use `cd` to move to the correct folder before beginning.

We can use `getwd()` to see our current working directory path and the files available using `list.file()` or `dir()`.

```{r chk_dir_files}
## what is the working directory?
getwd()

## what files exist in the working directory?
list.files()
dir()
```

Our current location is in the `week_04` sub-folder buried relatively deeply within the course `website` repository. 

Using `here()` returns the following information:

```{r here_ex}
## load here package
library(here)

## show files in base directory
list.files(here())

## show files in references directory
list.files(here("references"))
```

We can also use `here()` to check the path for specific files:

```{r}
here("data", "palmer_penguins.csv")
```

So the `here::here()` function creates a path *relative* to the folder/directory where the `.Rproj` file exists. 

## Checking for local files

You may run into situations where you'd like to 

1. check if a file already exists, and

2. if not, perhaps create a new folder/directory before downloading or saving the file.

The function `file.exists()` will test for the presence of a file and return `TRUE` or `FALSE`. For example,

`file.exists(here("my", "relative", "path", "filename.csv"))`

will look for the `filename.csv` within the folder/directory `/my/relative/path`.

For example, fitting statistical models in **JAGS** or **Stan** can take a lot of time (hours), and I don't want to have to refit the same models when doing an analysis. Therefore, I often use `file.exists()` within an `if` statement to first check if the folder/file exists, and if not,  create the folder and save the file.

```{r here_ex_save, eval = FALSE}
## set analysis directory
analysis_dir <- here("analysis")

## check if the `/analysis/cache` folder exists
## if not, create it
if(!file.exists(here("analysis_dir", "cache"))) {
  dir.create(here("analysis_dir", "cache"))
}

## check for saved file named `fit_ipm_JAGS.rds`
## if it doesn't exists, fit the model and save it
if(!file.exists(here("analysis_dir", "cache", "fit_ipm_JAGS.rds"))) {
  ## fit the model
  fit_ipm_JAGS <- rjags(...)
  ## save the model fit
  saveRDS(fit_ipm_JAGS, here("analysis_dir", "cache", "fit_ipm_JAGS.rds"))
}
```

# Getting data

## Downloading files

One option for reading data is to pull files directly from a website. Here is an example of downloading some river flow data from the U.S. Geological survey [National Water Information System](http://waterdata.usgs.gov/nwis).

We will use the direct link to the gauge data from the Skagit River near Mount Vernon, WA (gauge #12178000), beginning with the first year of fish data.

```{r get_flow_user_inputs}
## first & last years of flow data
yr_frst <- 2001
yr_last <- 2020

## flow gauge ID
flow_site <- 12178000
```

Here I break the URL into component pieces so its easier to see how and where the user input data are used.

```{r get_flow_url}
## get URL for flow data from USGS
flow_url <- paste0("https://waterdata.usgs.gov/nwis/dv",
                   "?cb_00060=on",
                   "&format=rdb",
                   "&site_no=",flow_site,
                   "&begin_date=",yr_frst,"-01-01",
                   "&end_date=",yr_last,"-12-31")
```

Next we retrieve the raw data file and print its metadata.

```{r get_flow_metadata, cache=TRUE}
## load `readr` package
library("readr")

## raw flow data from USGS
flow_raw <- read_lines(flow_url)

## lines with metadata
hdr_flow <- which(lapply(flow_raw, grep, pattern = "\\#")==1, arr.ind = TRUE)

## print flow metadata
print(flow_raw[hdr_flow], quote = FALSE)
```

Lastly, we extract the actual flow data for the years of interest and inspect the file contents.

```{r get_flows, cache=TRUE}
## flow data for years of interest
dat_flow <-  read_tsv(flow_url,
                      col_names = FALSE,
                      col_types = "ciDdc",
                      skip = max(hdr_flow)+2)

colnames(dat_flow) <- unlist(strsplit(tolower(flow_raw[max(hdr_flow)+1]),
                                      split = "\\s+"))
head(dat_flow)
```

We only need the 3rd and 4th columns, which contain the date (`datetime`) and daily flow measurements (``r grep("[0-9]$",colnames(dat_flow), value=TRUE)``). We will rename them to `date` and `flow`, respectively, and convert the flow units from "cubic feet per second" to "cubic meters per second".

```{r trim_dat_flow, cache=TRUE}
## keep only relevant columns
dat_flow <- dat_flow[c("datetime",
                       grep("[0-9]$", colnames(dat_flow), value = TRUE))]

## nicer column names
colnames(dat_flow) <- c("date","flow")

## convert cubic feet to cubic meters
dat_flow$flow <- dat_flow$flow / 35.3147

## flow by year & month
dat_flow$year <- as.integer(format(dat_flow$date,"%Y"))
dat_flow$month <- as.integer(format(dat_flow$date,"%m"))
dat_flow <- dat_flow[,c("year","month","flow")]

## inspect the file
head(dat_flow)
```



## Reading in CSV files

We also see there is a `palmer_penguins.csv` file in the `data` folder. Let's read it into R with the `readr` package. 

```{r here_ex_read}
data_raw <- readr::read_csv(here("lectures", "week_04", "data", "palmer_penguins.csv"))
data_raw
```

From there, we can read in the `palmer_penguins.csv`
like we have already learned how to do using the 
`readr::read_csv()` function: 

## Reading in a JSON file using `jsonlite`

### What is JSON? 

JSON (or JavaScript Object Notation) is a file
format that stores information in human-readable, 
organized, logical, easy-to-access manner.

For example, here is what a JSON file looks 
like: 

```{javascript, eval=FALSE}
var mark = {
  "city" : "Seattle",
  "state" : "WA", 
  "hobbies" : {
    "hobby1" : "cycling",
    "hobby2" : "skiing"
  }
  "bikes" : {
    "bike1" : "Ridley Helium",
    "bike2" : "Niner RLT",
    "bike3" : "Santa Cruz Tallboy"
  }
  "skis" : {
    "skis1" : "K2 Hardside",
    "skis2" : "DPS Wailer"
  }
}
```

Some features about `JSON` object: 

* JSON objects are surrounded by curly braces `{}`
* JSON objects are written in key/value pairs
* Keys must be strings, and values must be a valid JSON data type (string, number, object, array, boolean)
* Keys and values are separated by a colon
* Each key/value pair is separated by a comma

### Using GitHub API

Let's say we want to use the 
[GitHub API](https://developer.github.com/v3/?)
to find out how many of my GitHub repositories
have open issues? 

We will use the 
[jsonlite](https://cran.r-project.org/web/packages/jsonlite/index.html)
R package and the `fromJSON()` function
to convert from a JSON object to a data frame. 

We will read in a JSON file located at 
[https://api.github.com/users/mark_scheuerell/repos](https://api.github.com/users/mark_scheuerell/repos)

```{r github_ex}
github_url = "https://api.github.com/users/mark_scheuerell/repos"
library(jsonlite)
jsonData <- fromJSON(github_url)
```

The function `fromJSON()` has now converted 
the JSON file into a data frame with the names: 

```{r}
names(jsonData)
```

How many are private repos? How many have forks? 

```{r}
table(jsonData$private)
table(jsonData$forks)
```

What's the most popular language? 

```{r}
table(jsonData$language)
```

To find out how many repos that I have
with open issues, we can just create 
a table: 

```{r}
# how many repos have open issues? 
table(jsonData$open_issues_count)
```

Whew! Not as many as I thought.

How many do you have? 

Finally, I will leave you with a few 
other examples of using GitHub API: 

* [How long does it take to close a GitHub Issue in the `dplyr` package?](https://blog.exploratory.io/analyzing-issue-data-with-github-rest-api-63945017dedc)
* [How to retrieve all commits for a branch](https://stackoverflow.com/questions/9179828/github-api-retrieve-all-commits-for-all-branches-for-a-repo)
* [Getting my GitHub Activity](https://masalmon.eu/2017/12/21/wherehaveyoubeen/)

![](https://masalmon.eu/figure/source/2017-12-21-wherehaveyoubeen/unnamed-chunk-5-1.png)


## Reading in XML or HTML files using `rvest`

Do we want to purchase a book on Amazon? 

Next we are going to learn about what to do if
your data is on a website (XML or HTML) formatted 
to be read by humans instead of R.

We will use the (really powerful)
[rvest](https://cran.r-project.org/web/packages/rvest/rvest.pdf)
R package to do what is often called 
"scraping data from the web". 

Before we do that, we need to set up a 
few things:

* [SelectorGadget tool](http://selectorgadget.com/)
* [rvest and SelectorGadget guide](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html)
* [Awesome tutorial for CSS Selectors](http://flukeout.github.io/#)
* [Introduction to stringr](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html)
* [Regular Expressions/stringr tutorial](https://stat545-ubc.github.io/block022_regular-expression.html)
* [Regular Expression online tester](https://regex101.com/#python)- explains a regular expression as it is built, and confirms live whether and how it matches particular text.

We're going to be scraping [this page](http://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/product-reviews/0387981403/ref=cm_cr_dp_qt_see_all_top?ie=UTF8&showViewpoints=1&sortBy=helpful): it just contains the (first page of) reviews of the 
ggplot2 book by Hadley Wickham. 

```{r}
url <- "http://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/product-reviews/0387981403/ref=cm_cr_dp_qt_see_all_top?ie=UTF8&showViewpoints=1&sortBy=helpful"
```

We use the `rvest` package to download this page.

```{r}
library(rvest)
h <- read_html(url)
```

Now `h` is an `xml_document` that contains the contents of the page:

```{r}
h
```

How can you actually pull the interesting 
information out? That's where CSS selectors come in.

### CSS Selectors

CSS selectors are a way to specify a subset of 
nodes (that is, units of content) on a web page
(e.g., just getting the titles of reviews). 
CSS selectors are very powerful and not too 
challenging to master- here's 
[a great tutorial](http://flukeout.github.io/#) 
But honestly you can get a lot done even with 
very little understanding, by using a tool 
called SelectorGadget.

Install the [SelectorGadget](http://selectorgadget.com/) 
on your web browser. (If you use Chrome you can
use the Chrome extension, otherwise drag the 
provided link into your bookmarks bar). 
[Here's a guide for how to use it with rvest to "point-and-click" your way to a working selector](http://selectorgadget.com/).

For example, if you just wanted the titles, 
you'll end up with a selector that looks 
something like `.a-text-bold span`. You can pipe
your HTML object along with that selector 
into the `html_nodes` function, to select 
just those nodes:

```{r}
h %>%
  html_nodes(".a-text-bold span")
```

But you need the text from each of these, not the full tags. Pipe to the `html_text` function to pull these out:

```{r}
review_titles <- h %>%
  html_nodes(".a-text-bold span") %>%
  html_text()
review_titles
```

Now we've extracted something useful! Similarly, 
let's grab the format (hardcover or paperback).
Some experimentation with SelectorGadget 
shows it's:

```{r}
h %>%
  html_nodes(".a-size-mini.a-color-secondary") %>%
  html_text()
```

Now, we may be annoyed that it always
starts with `Format: `. Let's introduce 
the `stringr` package.

```{r}
formats <- h %>%
  html_nodes(".a-size-mini.a-color-secondary") %>%
  html_text() %>%
  stringr::str_replace("Format: ", "")
formats
```

We could do similar exercise for extracting
the number of stars and whether or not someone
found a review useful. This would help us decide
if we were interested in purchasing the book! 

# Summary

* Best practices for sharing data
* Best practices for downloading and reading in data
  * Relative versus absolute paths
  * Finding and creating files locally
* Best practices for getting data 
  * `jsonlite` for JSON (e.g. GitHub API)
  * `rvest` to grab all the exact elements you want (e.g. book reviews)
      * Check out selector gadget 
  * `DBI`, `RSQLite`, `dbplyr` for interacting with `SQLite` databses
  * Other APIs
      * Huffington Post API
    
## Other good R packages to know about 

* [`httr`](https://cran.r-project.org/web/packages/httr/index.html) for tools to work with URLs and HTTP
* [`googlesheets`](https://cran.r-project.org/web/packages/googlesheets/vignettes/basic-usage.html) to interact with Google Sheets in R
* [`googledrive`](https://googledrive.tidyverse.org](http://googledrive.tidyverse.org/) to interact with your Google Drive
